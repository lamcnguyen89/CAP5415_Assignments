{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAP 5415 Programming Assignment 03: Autoencoder\n",
    "\n",
    "\"\"\"\n",
    "Due Date: 1ONov2023\n",
    "Author: Lam Nguyen\n",
    "\n",
    "Subject: Autoencoder [2.5 pts]\n",
    "\n",
    "Overview:\n",
    "\n",
    "Implement autoencoder using MNIST dataset. The input size of the images will be 28x28 with a single channel. You will implement two different variations, one with fully connected layers and the other convolutional neural network.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. Implement an autoencoder using fully connected layers. \n",
    "    a. The encoder will have 2 layers (with 256, and 128 neurons)\n",
    "    b. The decoder will have 2 layers (with 256 and 784 neurons)\n",
    "    c. Train this network using MSE loss for 10 epochs\n",
    "    d. Compare the number of parameters  in the encoder and decoder.\n",
    "    e. Create a writeup:\n",
    "        i. Show 20 reconstructed images from testing data (2 image for each class)\n",
    "        ii. Show original images\n",
    "\n",
    "2. Implement an autoencoder using Convolutional layers. \n",
    "    a. The encoder will have 2 convolutional layers and 2 max pooling layers\n",
    "        i. Use kernel size 3x3\n",
    "        ii. reLU activation\n",
    "        iii. padding of 1 to preserve the feature map.\n",
    "    b. The decoder will have 3 convolutional layers\n",
    "        i. kernel shape is 3x3\n",
    "        ii. padding = 1\n",
    "        iii. The first 3 convolutional layers will be followed by an upsampling layer.\n",
    "                a. This upsampling layer will double the resolution of the feature maps using linear interpolation\n",
    "    c. Train the network for 10 epochs\n",
    "    d. Compare the number of parameters in the encoder and decoder.\n",
    "    e. Compare the total parameters in this autoencoder with the previous autoencoder.\n",
    "    f. Create Writeup:\n",
    "        i. Show 20 sample reconstructed images from testing data (2 images for each class)\n",
    "        ii. show original images\n",
    "        iii. Compare the reconstructed results with the previous autoencoder\n",
    "\n",
    "Note that you can choose any optimizer. Just use the same for both variations\n",
    "        \n",
    "\n",
    "Sources:\n",
    "\n",
    "Autoencoder in Pytorch-Theory and Implementation by Patrick Loeber: https://www.youtube.com/watch?v=zp8clK9yCro\n",
    "\n",
    "How to get info on model parameters using Torchinfo: https://pypi.org/project/torchinfo/\n",
    "\n",
    "How to save trained model: https://wandb.ai/wandb/common-ml-errors/reports/How-to-Save-and-Load-Models-in-PyTorch--VmlldzozMjg0MTE\n",
    "\n",
    "How to take subsets of dataset: https://discuss.pytorch.org/t/how-to-get-a-part-of-datasets/82161\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================#\n",
    "# 1. Load Modules\n",
    "# ========================================================================================#\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image\n",
    "import torch.optim as optim # Optimization algorithms\n",
    "import torch.nn as nn # All the Neural network models, loss functions\n",
    "import torch.nn.functional as F # All functions without parameters\n",
    "from torch.utils.data import DataLoader # Easier dataset management such as minibatches\n",
    "import torchvision.datasets as datasets # Standard datasets that can be used as test training data\n",
    "import torchvision.transforms as transforms # Transformations that can be performed on the dataset\n",
    "import torchvision.utils\n",
    "from torchinfo import summary # provides a summary of the model architecture and it's parameters\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import some packages for logging training and showing progress\n",
    "from tqdm_loggable.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Set up some basic logging to record traces of training\n",
    "logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        filename=\"Autoencoder_Documents/Autoencoder_Parameter_Summary.txt\" # Save log to a file\n",
    "    )\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28*28\n",
    "hidden_size = 100\n",
    "num_classes= 10\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "weight_decay = 1e-5\n",
    "    \n",
    "\n",
    "\n",
    "# Load GPU Parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================#\n",
    "# 2. Import Data:\n",
    "# =======================================================#\n",
    "\n",
    "train_dataset = datasets.MNIST(root='MNIST_dataset/', \n",
    "               train=True, \n",
    "               transform=transforms.ToTensor(),\n",
    "               download=True\n",
    "               )#Transforms transforms numpy array to tensors so that pytorch can use the data\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================#\n",
    "# 3. Create Fully Connected Autoencoder:\n",
    "# =======================================================#\n",
    "\n",
    "class FCC_Autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=28*28, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256,out_features=128),\n",
    "\n",
    "\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(128,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,28*28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "# Note: If images are in the range (-1,1) apply Tanh() activation instead of sigmoid\n",
    "\n",
    "\n",
    "# Input [-1, +1] -> use nn.Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================#\n",
    "# 3. Import the Fully Connected Autoencoder Model and train\n",
    "# ========================================================================================#\n",
    "\n",
    "model = FCC_Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "outputs = []\n",
    "for epoch in range(num_epochs):\n",
    "    for (img, _) in tqdm(train_loader):\n",
    "        img = img.to(device)\n",
    "        img = img.reshape(-1, 28*28) # -> use for Autoencoder_Linear\n",
    "        recon = model(img)\n",
    "        loss = criterion(recon, img)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch:{epoch+1}, Loss:{loss.item():.4f}')\n",
    "    outputs.append((epoch, img, recon))\n",
    "\n",
    "# =======================================================#\n",
    "# 6. Save the trained Fully Connected Autoencoder\n",
    "# =======================================================#\n",
    "\n",
    "print(\"Saving the Fully Connected Autoencoder Model to the folder: Assignment_03/01_Autoencoder/Trained_Autoencoders\")\n",
    "torch.save(model.state_dict(),'Trained_Autoencoders/FCC_Autoencoder_Model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================#\n",
    "# 5. Get the number of parameters for the FC-Autoencoder:\n",
    "# =======================================================#\n",
    "\n",
    "# Prints out the architecture of the trained model\n",
    "FCC_Autoencoder_summary = summary(model)\n",
    "print(FCC_Autoencoder_summary)\n",
    "\n",
    "logging.info(FCC_Autoencoder_summary) # Saves the parameter data file into the folder Autoencoder_Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for k in range(0, num_epochs, 4):\n",
    "    plt.figure(figsize=(9, 2))\n",
    "    plt.gray()\n",
    "    imgs = outputs[k][1].cpu().detach().numpy()\n",
    "    recon = outputs[k][2].cpu().detach().numpy()\n",
    "    for i, item in enumerate(imgs):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, i+1)\n",
    "        item = item.reshape(-1, 28,28) # -> use for Autoencoder_Linear\n",
    "        # item: 1, 28, 28\n",
    "        plt.imshow(item[0])\n",
    "            \n",
    "    for i, item in enumerate(recon):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, 9+i+1) # row_length + i + 1\n",
    "        item = item.reshape(-1, 28,28) # -> use for Autoencoder_Linear\n",
    "        # item: 1, 28, 28\n",
    "        plt.imshow(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================#\n",
    "# 6. Save the trained Fully Connected Autoencoder\n",
    "# =======================================================#\n",
    "\n",
    "print(\"Saving the Fully Connected Autoencoder Model to the folder: Assignment_03/01_Autoencoder/Trained_Autoencoders\")\n",
    "torch.save(model.state_dict(),'Trained_Autoencoders/FCC_Autoencoder_Model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        #N,1,28,28\n",
    "        super(CNN_Autoencoder,self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=3, padding=1), # N,16,10,10\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),  # N,16,5,5\n",
    "            nn.Conv2d(16, 8, 3, stride=2, padding=1), # N,8,3,3\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=1) # N,8,2,2\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "        nn.ConvTranspose2d(8, 16, 3, stride=2, padding=1), # N,16,3,3\n",
    "        nn.ReLU(),\n",
    "        nn.Upsample(scale_factor=2), # N, 16, 6, 6\n",
    "        nn.ConvTranspose2d(16, 8, 3, stride=1, padding=1), # N,8,6,6\n",
    "        nn.ReLU(),\n",
    "        nn.Upsample(scale_factor=2), # N,8,12,12\n",
    "        nn.ConvTranspose2d(8, 1, 3, stride=1, padding=1,dilation=2), # N,1,14,14\n",
    "        nn.Tanh(),\n",
    "        nn.Upsample(scale_factor=2) # N,1,28,28\n",
    "    )\n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        print(x.size())\n",
    "        x = self.decoder(x)\n",
    "        print(x.size())\n",
    "        return x\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================#\n",
    "# 2. Import the Convolutional Autoencoder Model and train\n",
    "# ========================================================================================#\n",
    "\n",
    "model = CNN_Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "outputs = []\n",
    "for epoch in range(num_epochs):\n",
    "    for (img, _) in tqdm(train_loader):\n",
    "\n",
    "        img = img.to(device)\n",
    "        # img = img.reshape(-1, 28*28) # -> use for Autoencoder_Linear\n",
    "        recon = model(img)\n",
    "        loss = criterion(recon, img)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(recon.size())\n",
    "\n",
    "    print(f'Epoch:{epoch+1}, Loss:{loss.item():.4f}')\n",
    "    outputs.append((epoch, img, recon))\n",
    "\n",
    "# =======================================================#\n",
    "# Save the trained Convolutional Autoencoder\n",
    "# =======================================================#\n",
    "\n",
    "\n",
    "print(\"Saving the Convolutional Autoencoder Model to the folder: Assignment_03/01_Autoencoder/Trained_Autoencoders\")\n",
    "torch.save(model.state_dict(),'Trained_Autoencoders/CNN_Autoencoder_Model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================#\n",
    "# 5. Get the number of parameters for the CNN-Autoencoder:\n",
    "# =======================================================#\n",
    "\n",
    "# Prints out the architecture of the trained model\n",
    "CNN_Autoencoder_summary = summary(model)\n",
    "print(CNN_Autoencoder_summary)\n",
    "\n",
    "logging.info(CNN_Autoencoder_summary) # Saves the parameter data file into the folder Autoencoder_Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for k in range(0, num_epochs, 4):\n",
    "    plt.figure(figsize=(9, 2))\n",
    "    plt.gray()\n",
    "    imgs = outputs[k][1].cpu().detach().numpy()\n",
    "    recon = outputs[k][2].cpu().detach().numpy()\n",
    "    for i, item in enumerate(imgs):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, i+1)\n",
    "        # item = item.reshape(-1, 28,28) # -> use for Autoencoder_Linear\n",
    "        # item: 1, 28, 28\n",
    "        plt.imshow(item[0])\n",
    "            \n",
    "    for i, item in enumerate(recon):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, 9+i+1) # row_length + i + 1\n",
    "        # item = item.reshape(-1, 28,28) # -> use for Autoencoder_Linear\n",
    "        # item: 1, 28, 28\n",
    "        plt.imshow(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 28])\n",
      "torch.Size([1, 5, 28])\n",
      "torch.Size([1, 5, 28])\n",
      "torch.Size([1, 5, 28])\n",
      "torch.Size([1, 5, 28])\n",
      "torch.Size([1, 5, 28])\n",
      "torch.Size([1, 5, 28])\n",
      "torch.Size([1, 5, 28])\n",
      "torch.Size([1, 5, 28])\n",
      "torch.Size([1, 5, 28])\n",
      "torch.Size([1, 5, 28])\n",
      "torch.Size([1, 5, 28])\n",
      "torch.Size([1, 5, 28])\n",
      "torch.Size([1, 5, 28])\n",
      "torch.Size([1, 5, 28])\n",
      "torch.Size([1, 5, 28])\n",
      "torch.Size([1, 5, 28])\n",
      "torch.Size([1, 5, 28])\n",
      "torch.Size([1, 5, 28])\n",
      "torch.Size([1, 5, 28])\n"
     ]
    }
   ],
   "source": [
    "# Input 20 images into CNN Autoencoder\n",
    "\n",
    "input_images_array= Path(\"Test_Images\").glob('*.png')\n",
    "\n",
    "model = CNN_Autoencoder().to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('Trained_Autoencoders/CNN_Autoencoder_Model.pth'))\n",
    "\n",
    "model.eval()\n",
    "counter=0\n",
    "for image in input_images_array:\n",
    "    counter+=1\n",
    "    im = Image.open(image).convert(\"L\")\n",
    "    im = np.asarray(im)\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    tensor_img = convert_tensor(im).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        processed_img = model(tensor_img)\n",
    "        print(processed_img.size())\n",
    "        save_image(processed_img, f'Processed_CNN_Images/processed_{counter}.jpg')\n",
    "\n",
    "\n",
    "# Why the hell are they coming out the wrong size?  \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Input 20 images into FCC Autoencoder\n",
    "\n",
    "input_images_array= Path(\"Test_Images\").glob('*.png')\n",
    "\n",
    "model = FCC_Autoencoder().to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('Trained_Autoencoders/FCC_Autoencoder_Model.pth'))\n",
    "\n",
    "model.eval()\n",
    "counter=0\n",
    "for image in input_images_array:\n",
    "    counter+=1\n",
    "    im = Image.open(image).convert(\"L\")\n",
    "    im = np.asarray(im)\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    im = convert_tensor(im)\n",
    "    im = im.to(device)\n",
    "    im = im.reshape(-1,28*28)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        processed_img = model(im)\n",
    "        processed_img = processed_img.reshape(-1,28,28)\n",
    "        print(processed_img.size())\n",
    "        save_image(processed_img, f'Processed_FCC_Images/processed_{counter}.jpg')\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
