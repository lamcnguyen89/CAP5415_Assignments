{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDesign a CNN architecture which has more then 2 convolutional layers and more then 1 fully connected layers. It should make 10 predictions for the 10 classes of CIFAR-10.\\n\\n1. Train this network on CIFAR-10 for 30 epochs\\n2. Use Cross-Entropy Loss\\n3. SGD Optimizer\\n4. Softmax activation after the final fully connected layer.\\n5. Report Training/Testing Loss after each epoch in the form of plots and accuracy scores after 30 epochs.\\n\\n6. Finally, increase the number of conv layers in the above network and train again. Report the same numbers and plots again comparing with the first network.\\n\\n\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CAP 5415 Programming Assignment 04: Image Classification\n",
    "\n",
    "\"\"\"\n",
    "Design a CNN architecture which has more then 2 convolutional layers and more then 1 fully connected layers. It should make 10 predictions for the 10 classes of CIFAR-10.\n",
    "\n",
    "1. Train this network on CIFAR-10 for 30 epochs\n",
    "2. Use Cross-Entropy Loss\n",
    "3. SGD Optimizer\n",
    "4. Softmax activation after the final fully connected layer.\n",
    "5. Report Training/Testing Loss after each epoch in the form of plots and accuracy scores after 30 epochs.\n",
    "\n",
    "6. Finally, increase the number of conv layers in the above network and train again. Report the same numbers and plots again comparing with the first network.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================#\n",
    "# 1. Import basic Modules and Functions and set variables\n",
    "# =======================================================#\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn # All the Neural network models, loss functions\n",
    "import torch.optim as optim # Optimization algorithms\n",
    "import torch.nn.functional as F # All functions without parameters\n",
    "from torch.utils.data import DataLoader # Easier dataset management such as minibatches\n",
    "import torchvision.datasets as datasets # Standard datasets that can be used as test training data\n",
    "import torchvision.transforms as transforms # Transformations that can be performed on the dataset\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Import some packages for logging training and showing progress\n",
    "from tqdm_loggable.auto import tqdm\n",
    "from tqdm_loggable.tqdm_logging import tqdm_logging\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "\n",
    "# Set up some basic logging to record traces of training\n",
    "logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        filename=\"output/output.txt\" # Save log to a file\n",
    "    )\n",
    "\n",
    "tqdm_logging.set_level(logging.INFO)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_channels = 3\n",
    "hidden_size = 100\n",
    "num_classes= 10\n",
    "learning_rate = 0.1\n",
    "batch_size = 10\n",
    "num_epochs = 30\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================#\n",
    "# 2. Create a Convolutional Neural Network:\n",
    "# =========================================#\n",
    "\n",
    "class NN_2(nn.Module):\n",
    "    def __init__(self,input_channels,hidden_size, num_classes):\n",
    "        super(NN_2, self).__init__() # The Super keyword calls the initialization of the parent class\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels,\n",
    "                               out_channels=40,\n",
    "                               kernel_size=(5,5),\n",
    "                               stride=(1,1),\n",
    "                               padding=(0,0)\n",
    "                               )\n",
    "        self.conv2 = nn.Conv2d(in_channels=40,\n",
    "                               out_channels=40,\n",
    "                               kernel_size=(5,5),\n",
    "                               stride=(1,1),\n",
    "                               padding=(0,0)\n",
    "                               )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,2),\n",
    "                                  stride=(2,2)\n",
    "        ) \n",
    "        self.fc1 = nn.Linear(40*5*5, hidden_size) \n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def num_flat_features(self,x):\n",
    "        size = x.size()[1:] # all dimensions except batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *=s\n",
    "\n",
    "        return num_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.sigmoid(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1,self.num_flat_features(x))\n",
    "        x = F.softmax(self.fc1(x),-1)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# =======================================================#\n",
    "# 3. Load and prepare dataset:\n",
    "# =======================================================#\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='dataset/', \n",
    "               train=True, \n",
    "               transform=transforms.ToTensor(),\n",
    "               download=True\n",
    "               )#Transforms transforms numpy array to tensors so that pytorch can use the data\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='dataset/', \n",
    "               train=False, \n",
    "               transform=transforms.ToTensor(),\n",
    "               download=True\n",
    "               )#Transforms transforms numpy array to tensors so that pytorch can use the data\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset= test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================#\n",
    "# 4. Define Accuracy Function:\n",
    "# =======================================================#\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"Checking accuracy on training data\")\n",
    "        logging.info(\"Checking accuracy on training data\")\n",
    "    else:\n",
    "        print(\"Checking accuracy on test data\")\n",
    "        logging.info(\"Checking accuracy on test data\")\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad(): # No gradients have to be calculated\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            scores = model(x)\n",
    "            _,predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples) * 100:.2f}')\n",
    "        logging.info(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples) * 100:.2f}')\n",
    "\n",
    "    model.train()\n",
    "    acc = float(num_correct)/float(num_samples)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/2 Training Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:15<00:00, 328.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on training data\n",
      "Got 9578 / 50000 with accuracy 19.16\n",
      "Step 1/2 Training Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:14<00:00, 337.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on training data\n",
      "Got 13605 / 50000 with accuracy 27.21\n",
      "Step 1/2 Training Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:25<00:00, 194.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on training data\n",
      "Got 13671 / 50000 with accuracy 27.34\n",
      "Step 1/2 Training Epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:52<00:00, 44.27it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on training data\n",
      "Got 11951 / 50000 with accuracy 23.90\n",
      "Step 1/2 Training Epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1258/5000 [00:25<01:15, 49.28it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/malneyugnfl/softwaredev/CAP5415_Assignments/Assignment_04/Image_Classification.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/malneyugnfl/softwaredev/CAP5415_Assignments/Assignment_04/Image_Classification.ipynb#W5sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# Go Backward in the network:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/malneyugnfl/softwaredev/CAP5415_Assignments/Assignment_04/Image_Classification.ipynb#W5sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/malneyugnfl/softwaredev/CAP5415_Assignments/Assignment_04/Image_Classification.ipynb#W5sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/malneyugnfl/softwaredev/CAP5415_Assignments/Assignment_04/Image_Classification.ipynb#W5sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Gradient descent or adam step\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/malneyugnfl/softwaredev/CAP5415_Assignments/Assignment_04/Image_Classification.ipynb#W5sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/softwaredev/anaconda3/envs/deeplearning/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/softwaredev/anaconda3/envs/deeplearning/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =======================================================#\n",
    "# 5. Train the Convolutional Neural Network:\n",
    "# =======================================================#\n",
    "\n",
    "\n",
    "\n",
    "#Initialize Model\n",
    "model = NN_2(\n",
    "    input_channels=input_channels,\n",
    "    hidden_size=hidden_size,\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "logging.info(f\"Begin Training CIFAR-10 dataset with this model: {model}\")\n",
    "\n",
    "# Define the loss function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=learning_rate)\n",
    "\n",
    "# Set Some state variables\n",
    "epoch_counter= 0\n",
    "current_loss = 1\n",
    "current_accuracy = 1\n",
    "model_accuracy = []\n",
    "model_loss = []\n",
    "\n",
    "\n",
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch > 0:\n",
    "        current_accuracy = check_accuracy(train_loader,model)\n",
    "        model_accuracy.append(current_accuracy)\n",
    "        model_loss.append(round(current_loss, 4))\n",
    "    tqdm.write(f\"Step 1/2 Training Epoch {epoch+1}/{num_epochs}\")\n",
    "    # store points for for plotting the accuracy and loss:\n",
    "    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n",
    "\n",
    "        \n",
    "        # Get data to Cuda/gpu if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets = targets.to(device=device)\n",
    "    \n",
    "        # Forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # Go Backward in the network:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss = loss.item()\n",
    "        # logging.info(\"Training single layer Neural Network \")\n",
    "        if epoch > epoch_counter+4:\n",
    "            logging.info(f\"Training Epoch: {epoch}, loss = {loss.item():.4f}\")\n",
    "\n",
    "            epoch_counter = epoch\n",
    "\n",
    "\n",
    "epoch_counter = 0\n",
    "\n",
    "print(f'Model Accuracy: {model_accuracy}')\n",
    "print(f'Model Loss: {model_loss}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(),'trained_models/Classifier_01.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================================#\n",
    "# 7. Graph Training/Testing Loss after each epoch in the form of plots and accuracy scores after 30 epochs:\n",
    "# =========================================================================================================#\n",
    "\n",
    "epoch_list = []\n",
    "\n",
    "for i in range(len(model_accuracy)):\n",
    "    epoch_list.append(i)\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.plot(epoch_list, model_accuracy)\n",
    "plt.title(\"Training Accuracy Change over Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Loss\n",
    "plt.plot(epoch_list, model_loss)\n",
    "plt.title(\"Training Loss Change over Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================================#\n",
    "# 8. Increase the number of convolutional layers in the network:\n",
    "# =========================================================================================================#\n",
    "\n",
    "class NN_3(nn.Module):\n",
    "    def __init__(self,input_channels,hidden_size, num_classes):\n",
    "        super(NN_3, self).__init__() # The Super keyword calls the initialization of the parent class\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels,\n",
    "                               out_channels=40,\n",
    "                               kernel_size=(5,5),\n",
    "                               stride=(1,1),\n",
    "                               padding=(0,0)\n",
    "                               )\n",
    "        self.conv2 = nn.Conv2d(in_channels=40,\n",
    "                               out_channels=40,\n",
    "                               kernel_size=(5,5),\n",
    "                               stride=(1,1),\n",
    "                               padding=(0,0)\n",
    "                               )\n",
    "        self.conv3 = nn.Conv2d(in_channels=40,\n",
    "                               out_channels=40,\n",
    "                               kernel_size=(2,2),\n",
    "                               stride=(1,1),\n",
    "                               padding=(0,0)\n",
    "                               )\n",
    "        self.conv4 = nn.Conv2d(in_channels=40,\n",
    "                               out_channels=40,\n",
    "                               kernel_size=(2,2),\n",
    "                               stride=(1,1),\n",
    "                               padding=(0,0)\n",
    "                               )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,2),\n",
    "                                  stride=(2,2)\n",
    "        ) \n",
    "        self.fc1 = nn.Linear(40*3*3, hidden_size) \n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def num_flat_features(self,x):\n",
    "        size = x.size()[1:] # all dimensions except batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *=s\n",
    "\n",
    "        return num_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.sigmoid(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.sigmoid(self.conv3(x))\n",
    "        x = F.sigmoid(self.conv4(x))\n",
    "        x = x.view(-1,self.num_flat_features(x))\n",
    "        x = F.softmax(self.fc1(x),-1)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================#\n",
    "# 9. Train the Updated Convolutional Neural Network:\n",
    "# =======================================================#\n",
    "\n",
    "#Initialize Model\n",
    "model = NN_3(\n",
    "    input_channels=input_channels,\n",
    "    hidden_size=hidden_size,\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "logging.info(f\"Begin Training CIFAR-10 dataset with this model: {model}\")\n",
    "\n",
    "# Define the loss function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=learning_rate)\n",
    "\n",
    "# Set Some state variables\n",
    "epoch_counter= 0\n",
    "current_loss = 1\n",
    "current_accuracy = 1\n",
    "model_accuracy = []\n",
    "model_loss = []\n",
    "\n",
    "\n",
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch > 0:\n",
    "        current_accuracy = check_accuracy(train_loader,model)\n",
    "        model_accuracy.append(current_accuracy)\n",
    "        model_loss.append(round(current_loss, 4))\n",
    "    tqdm.write(f\"Step 1/2 Training Epoch {epoch+1}/{num_epochs}\")\n",
    "    # store points for for plotting the accuracy and loss:\n",
    "    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n",
    "\n",
    "        \n",
    "        # Get data to Cuda/gpu if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets = targets.to(device=device)\n",
    "    \n",
    "        # Forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # Go Backward in the network:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss = loss.item()\n",
    "        # logging.info(\"Training single layer Neural Network \")\n",
    "        if epoch > epoch_counter+4:\n",
    "            logging.info(f\"Training Epoch: {epoch}, loss = {loss.item():.4f}\")\n",
    "\n",
    "            epoch_counter = epoch\n",
    "\n",
    "\n",
    "epoch_counter = 0\n",
    "\n",
    "print(f'Model Accuracy: {model_accuracy}')\n",
    "print(f'Model Loss: {model_loss}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(),'trained_models/Classifier_02.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================================#\n",
    "# 10. Graph Training/Testing Loss after each epoch in the form of plots and accuracy scores after 30 epochs:\n",
    "# =========================================================================================================#\n",
    "\n",
    "epoch_list = []\n",
    "\n",
    "for i in range(len(model_accuracy)):\n",
    "    epoch_list.append(i)\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.plot(epoch_list, model_accuracy)\n",
    "plt.title(\"Training Accuracy Change over Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Loss\n",
    "plt.plot(epoch_list, model_loss)\n",
    "plt.title(\"Training Loss Change over Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
