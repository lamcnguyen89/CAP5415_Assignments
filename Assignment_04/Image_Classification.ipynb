{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAP 5415 Programming Assignment 04: Image Classification\n",
    "\n",
    "\"\"\"\n",
    "Design a CNN architecture which has more then 2 convolutional layers and more then 1 fully connected layers. It should make 10 predictions for the 10 classes of CIFAR-10.\n",
    "\n",
    "1. Train this network on CIFAR-10 for 30 epochs\n",
    "2. Use Cross-Entropy Loss\n",
    "3. SGD Optimizer\n",
    "4. Softmax activation after the final fully connected layer.\n",
    "5. Report Training/Testing Loss after each epoch in the form of plots and accuracy scores after 30 epochs.\n",
    "\n",
    "6. Finally, increase the number of conv layers in the above network and train again. Report the same numbers and plots again comparing with the first network.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================#\n",
    "# 1. Import basic Modules and Functions and set variables\n",
    "# =======================================================#\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn # All the Neural network models, loss functions\n",
    "import torch.optim as optim # Optimization algorithms\n",
    "import torch.nn.functional as F # All functions without parameters\n",
    "from torch.utils.data import DataLoader # Easier dataset management such as minibatches\n",
    "import torchvision.datasets as datasets # Standard datasets that can be used as test training data\n",
    "import torchvision.transforms as transforms # Transformations that can be performed on the dataset\n",
    "\n",
    "\n",
    "# Import some packages for logging training and showing progress\n",
    "from tqdm_loggable.auto import tqdm\n",
    "from tqdm_loggable.tqdm_logging import tqdm_logging\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "\n",
    "# Set up some basic logging to record traces of training\n",
    "logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        filename=\"Assignment_02/output/Step_02_output.txt\" # Save log to a file\n",
    "    )\n",
    "\n",
    "tqdm_logging.set_level(logging.INFO)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_channels = 1\n",
    "hidden_size = 100\n",
    "num_classes= 10\n",
    "learning_rate = 0.1\n",
    "batch_size = 10\n",
    "num_epochs = 30\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================#\n",
    "# 2. Next insert two Convolutional Laters to the network built in previous step:\n",
    "# ================================================================================#\n",
    "\n",
    "class NN_2(nn.Module):\n",
    "    def __init__(self,input_channels,hidden_size, num_classes):\n",
    "        super(NN_2, self).__init__() # The Super keyword calls the initialization of the parent class\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels,\n",
    "                               out_channels=40,\n",
    "                               kernel_size=(5,5),\n",
    "                               stride=(1,1),\n",
    "                               padding=(0,0)\n",
    "                               )\n",
    "        self.conv2 = nn.Conv2d(in_channels=40,\n",
    "                               out_channels=40,\n",
    "                               kernel_size=(5,5),\n",
    "                               stride=(1,1),\n",
    "                               padding=(0,0)\n",
    "                               )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,2),\n",
    "                                  stride=(2,2)\n",
    "        ) \n",
    "        self.fc1 = nn.Linear(40*4*4, hidden_size) \n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def num_flat_features(self,x):\n",
    "        size = x.size()[1:] # all dimensions except batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *=s\n",
    "\n",
    "        return num_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.sigmoid(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        #x = x.reshape(x.shape[0], -1)\n",
    "        x = x.view(-1,self.num_flat_features(x))\n",
    "        x = F.softmax(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================#\n",
    "# 3. Train the Convolutional Neural Network:\n",
    "# =======================================================#\n",
    "\n",
    "\n",
    "# Prepare the data for processing through the Network:\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='Assignment_04/dataset/', \n",
    "               train=True, \n",
    "               transform=transforms.ToTensor(),\n",
    "               download=True\n",
    "               )#Transforms transforms numpy array to tensors so that pytorch can use the data\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='Assignment_04/dataset/', \n",
    "               train=False, \n",
    "               transform=transforms.ToTensor(),\n",
    "               download=True\n",
    "               )#Transforms transforms numpy array to tensors so that pytorch can use the data\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset= test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "#Initialize Model\n",
    "model = NN_2(\n",
    "    input_channels=input_channels,\n",
    "    hidden_size=hidden_size,\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "logging.info(f\"Begin Training MNIST dataset with this model: {model}\")\n",
    "\n",
    "# Define the loss function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=learning_rate)\n",
    "\n",
    "epoch_counter= 0\n",
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    tqdm.write(f\"Step 2/5 Training Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n",
    "\n",
    "        \n",
    "        # Get data to Cuda/gpu if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets = targets.to(device=device)\n",
    "    \n",
    "        # Forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # Go Backward in the network:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "        \n",
    "        #logging.info(\"Training single layer Neural Network \")\n",
    "        if epoch > epoch_counter+4:\n",
    "            logging.info(f\"Training Epoch: {epoch}, loss = {loss.item():.4f}\")\n",
    "\n",
    "            epoch_counter = epoch\n",
    "\n",
    "\n",
    "epoch_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================#\n",
    "# 4. Check Accuracy on training and test to see the accuracy of the model:\n",
    "# =========================================================================#\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"Checking accuracy on training data\")\n",
    "        logging.info(\"Checking accuracy on training data\")\n",
    "    else:\n",
    "        print(\"Checking accuracy on test data\")\n",
    "        logging.info(\"Checking accuracy on test data\")\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad(): # No gradients have to be calculated\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            scores = model(x)\n",
    "            _,predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples) * 100:.2f}')\n",
    "        logging.info(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples) * 100:.2f}')\n",
    "\n",
    "    model.train()\n",
    "    acc = num_correct/num_samples\n",
    "    return acc\n",
    "\n",
    "check_accuracy(train_loader,model)\n",
    "check_accuracy(test_loader,model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
